{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d4bb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- RateCodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|  pickup_longitude|   pickup_latitude|RateCodeID|store_and_fwd_flag| dropoff_longitude|  dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|       2| 2015-01-15 19:05:39|  2015-01-15 19:23:42|              1|         1.59|  -73.993896484375|  40.7501106262207|         1|                 N|-73.97478485107422| 40.75061798095703|           1|       12.0|  1.0|    0.5|      3.25|         0.0|                  0.3|       17.05|\n",
      "|       1| 2015-01-10 20:33:38|  2015-01-10 20:53:28|              1|          3.3|-74.00164794921875|  40.7242431640625|         1|                 N|-73.99441528320312| 40.75910949707031|           1|       14.5|  0.5|    0.5|       2.0|         0.0|                  0.3|        17.8|\n",
      "|       1| 2015-01-10 20:33:38|  2015-01-10 20:43:41|              1|          1.8|-73.96334075927734| 40.80278778076172|         1|                 N|-73.95182037353516| 40.82441329956055|           2|        9.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        10.8|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:35:31|              1|          0.5|-74.00908660888672| 40.71381759643555|         1|                 N|-74.00432586669922| 40.71998596191406|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:52:58|              1|          3.0|-73.97117614746094|40.762428283691406|         1|                 N|-74.00418090820312|40.742652893066406|           2|       15.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        16.3|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 15:10:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/11/27 15:10:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree F1 Score: 0.9616559334817972\n",
      "Logistic Regression F1 Score: 0.8317299367110219\n",
      "Best Model: Decision Tree\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, dayofweek, to_timestamp, when, col, sqrt, pow\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HighFarePrediction\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Task 1: Data Preparation\n",
    "# a) Load Dataset\n",
    "df = spark.read.csv(\"yellow_tripdata_2015-01.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# b) Data Exploration\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "df = df.dropna()\n",
    "\n",
    "# c) Feature Engineering\n",
    "df = df.withColumn(\"pickup_hour\", hour(to_timestamp(df[\"tpep_pickup_datetime\"])))\n",
    "df = df.withColumn(\"pickup_day\", dayofweek(to_timestamp(df[\"tpep_pickup_datetime\"])))\n",
    "df = df.withColumn(\n",
    "    \"distance\",\n",
    "    sqrt(\n",
    "        pow(col(\"pickup_longitude\") - col(\"dropoff_longitude\"), 2) +\n",
    "        pow(col(\"pickup_latitude\") - col(\"dropoff_latitude\"), 2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# d) Target Variable Creation\n",
    "df = df.withColumn(\"high_fare\", when(df[\"fare_amount\"] > 20, 1).otherwise(0))\n",
    "\n",
    "# e) Data Splitting\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Task 2: Decision Tree Classifier Pipeline\n",
    "# a) Define the pipeline stages\n",
    "feature_cols = [\"pickup_hour\", \"pickup_day\", \"distance\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "dt = DecisionTreeClassifier(labelCol=\"high_fare\", featuresCol=\"scaled_features\")\n",
    "\n",
    "pipeline_dt = Pipeline(stages=[assembler, scaler, dt])\n",
    "\n",
    "# b) Hyperparameter Tuning\n",
    "param_grid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 2, 4]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"high_fare\", metricName=\"f1\")\n",
    "\n",
    "cv_dt = CrossValidator(\n",
    "    estimator=pipeline_dt,\n",
    "    estimatorParamMaps=param_grid_dt,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "# c) Model Training\n",
    "model_dt = cv_dt.fit(train_data)\n",
    "\n",
    "# d) Model Evaluation\n",
    "predictions_dt = model_dt.transform(test_data)\n",
    "f1_dt = evaluator.evaluate(predictions_dt)\n",
    "\n",
    "# e) Save Pipeline\n",
    "model_dt.write().overwrite().save(\"decision_tree_pipeline\")\n",
    "\n",
    "# Task 3: Logistic Regression Pipeline\n",
    "# a) Define the pipeline\n",
    "lr = LogisticRegression(labelCol=\"high_fare\", featuresCol=\"scaled_features\")\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "# b) Hyperparameter Tuning\n",
    "param_grid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "cv_lr = CrossValidator(\n",
    "    estimator=pipeline_lr,\n",
    "    estimatorParamMaps=param_grid_lr,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "# c) Model Training\n",
    "model_lr = cv_lr.fit(train_data)\n",
    "\n",
    "# d) Model Evaluation\n",
    "predictions_lr = model_lr.transform(test_data)\n",
    "f1_lr = evaluator.evaluate(predictions_lr)\n",
    "\n",
    "# Save Logistic Regression Pipeline\n",
    "model_lr.write().overwrite().save(\"logistic_regression_pipeline\")\n",
    "\n",
    "# Task 4: Report Findings\n",
    "# a) Discuss performance\n",
    "results = f\"\"\"\n",
    "Decision Tree F1 Score: {f1_dt}\n",
    "Logistic Regression F1 Score: {f1_lr}\n",
    "Best Model: {'Decision Tree' if f1_dt > f1_lr else 'Logistic Regression'}\n",
    "\"\"\"\n",
    "\n",
    "# b) State the best-performing pipeline\n",
    "print(results)\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
